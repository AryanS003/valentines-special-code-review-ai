# -*- coding: utf-8 -*-
"""hackerrank-web-scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18LgrKqLfphvJZozR35t0lb6IiHnr_mFa
"""

import requests
import re
from bs4 import BeautifulSoup
import streamlit

response = requests.get("https://www.hackerrank.com/login")

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.hackerrank.com/",
    "Accept-Language": "en-US,en;q=0.9",
}

session = requests.Session()
login_url = "https://www.hackerrank.com/login"

payload = {
    "login" = st.secrets["credentials"]["login"],
    "password" = st.secrets["credentials"]["password"]
}

response = session.post(login_url, headers=headers, data=payload)
print(response.status_code)  # 200

key_val_text = response.text
pattern = r'"csrf_token"\s*:\s*"(.*?)"'

# pattern search
match = re.search(pattern, key_val_text)

if match:
    csrf_token = match.group(1)
else:
    st.warning("CSRF token not found, sorry")


    





if response.status_code == 200:
    # access protected content
    content_response = session.get('https://www.hackerrank.com/domains/python?filters%5Bsubdomains%5D%5B%5D=py-math')
else:
    st.warning('Login failed')
    
# Step 1: Get the CSRF token from the login page
login_page = session.get(login_url, headers=headers)
soup = BeautifulSoup(login_page.text, "html.parser")
# Step 2: Login with CSRF token
payload = {
    "login" = st.secrets["credentials"]["login"],
    "password" = st.secrets["credentials"]["password"],
    "csrf_token": csrf_token  # Include CSRF token
}

headers["X-CSRF-Token"] = csrf_token  # Also add it to headers

response = session.post(login_url, headers=headers, data=payload)
print(response.status_code)  # 200 means login successful

# Step 3: Access protected page after successful login
content_url = "https://www.hackerrank.com/domains/python?filters%5Bsubdomains%5D%5B%5D=py-math"
content_response = session.get(content_url, headers=headers)
print(content_response.status_code)

# print(content_response.text)


soup = BeautifulSoup(content_response.text, "html.parser")

base_url = "https://www.hackerrank.com"
filtered_links = [
    base_url + link["href"]
    for link in soup.find_all("a", href=True)
    if link["href"].startswith("/challenges")
]
new_links = []
for link in filtered_links:
    new_links.append(link.split()[0])



